# Loan Approval Optimization — Deep Learning + Offline Reinforcement Learning

This repository provides a complete end-to-end workflow for **loan approval optimization** using both **Deep Learning (DL)** and **Offline Reinforcement Learning (RL)** techniques.  

The objective is to enable a fintech institution to **maximize profitability** while **minimizing default risk**, by learning optimal approval policies from historical data.

---

## 🧭 Overview

The project builds and compares two complementary approaches:

- **Deep Learning (DL):** Predicts the probability of loan default using a neural network model.  
- **Offline Reinforcement Learning (RL):** Learns a reward-driven approval policy using Conservative Q-Learning (CQL).  

The overall workflow is structured into four major tasks:

1. **Task 1 – Exploratory Data Analysis (EDA) & Preprocessing**  
   - Cleans and transforms the raw LendingClub dataset.  
   - Performs feature engineering and encoding.  
   - Outputs a standardized dataset for modeling.

2. **Task 2 – Deep Learning Default-Risk Prediction (Keras MLP)**  
   - Builds and trains a binary classifier to predict loan default probability.  
   - Evaluates performance using **AUC** and **F1-Score**.

3. **Task 3 – Offline Reinforcement Learning (CQL via d3rlpy)**  
   - Frames loan approval as a sequential decision problem.  
   - Uses a static dataset to learn a policy that maximizes long-term financial return.  

4. **Task 4 – Analysis, Comparison, and Future Directions**  
   - Compares DL and RL results, analyzing both predictive and policy-based performance.  
   - Highlights disagreement cases and proposes future enhancements.

---

## 📂 Folder Structure

```
.
├── input/
│   └── accepted_2007_to_2018Q4.csv       # Raw dataset (Kaggle)
├── pranjal-dl-eda.ipynb                  # Task 1 & 2: EDA + Deep Learning Model
├── reinforce-pranjal.ipynb               # Task 3: Offline RL Agent
├── Final_Report.pdf                      # Task 4: Analysis & Comparison
├── requirements.txt                      # Dependencies
└── README.md                             # Project documentation
```

---

## ⚙️ Environment Setup

### Option A — Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install -U pip
pip install -r requirements.txt
```

### Option B — Conda

```bash
conda create -n loan-rl python=3.11 -y
conda activate loan-rl
pip install -r requirements.txt
```

---

## 🧩 Data Files

Place the following files inside the `input/` directory:

- **Preferred:** `loan_clean_subset.csv` (generated by Task 1)  
  Contains feature columns and target label (`0 = Fully Paid`, `1 = Defaulted`).

- **Optional:** `accepted_2007_to_2018Q4.csv`  
  If present, Task 1 automatically cleans and produces `loan_clean_subset.csv`.

Ensure `loan_amnt` and `int_rate` columns are present for RL reward computation.

---

## 🚀 How to Run

### Step 1 — EDA & Preprocessing

Notebook: `pranjal-dl-eda.ipynb`  
Tasks performed:
- Load and clean raw data
- Feature engineering and scaling
- Save `loan_clean_subset.csv`

### Step 2 — Deep Learning Model

Notebook: `pranjal-dl-eda.ipynb`  
Tasks performed:
- Train a PyTorch/Keras MLP model
- Evaluate ROC-AUC and F1-Score

**Outputs:**
- Trained model weights
- Metrics summary and confusion matrix

### Step 3 — Offline Reinforcement Learning

Notebook: `reinforce-pranjal.ipynb`  
Tasks performed:
- Create an `MDPDataset` with approve/deny actions
- Train a CQL agent using d3rlpy
- Evaluate policy via Estimated Policy Value (EPV)

**Outputs:**
- RL model files (`.d3`, `.joblib`)
- EPV, approval rate, and decision matrix

### Step 4 — Analysis & Comparison

Notebook: `Task4_Analysis_Comparison.ipynb`  
Tasks performed:
- Compare DL (AUC/F1) vs RL (EPV)
- Identify disagreement cases
- Generate summary report

**Outputs:**
- Metrics table (AUC, F1, EPV)
- Disagreement examples
- Final summary block (PDF or notebook cell)

---

## 📊 Expected Results

| Model Type | Metric | Value |
|-------------|---------|--------|
| Deep Learning | F1-Score | 0.747 |
| Deep Learning | AUC | 0.928 |
| Offline RL | Estimated Policy Value | Variable (depends on dataset) |

---

## 🧠 Insights

- The DL model focuses on minimizing default probability.  
- The RL agent optimizes **expected financial reward**, balancing approval rate and risk.  
- Combined analysis demonstrates how **Offline RL** can complement supervised learning in financial decision-making.

---

## 🧾 requirements.txt

```
numpy<2.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
joblib>=1.3.0

# Deep Learning (PyTorch)
torch==2.4.1

# Offline RL (Conservative Q-Learning) & dependencies
d3rlpy==2.4.0
gymnasium[classic-control]==0.29.1

# Jupyter kernel support (optional)
ipykernel>=6.29.0
```

---

## ⚖️ License

This repository is provided for educational and internal research purposes only.
