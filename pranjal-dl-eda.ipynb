{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13546623,"sourceType":"datasetVersion","datasetId":8603326}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:46:58.167256Z","iopub.execute_input":"2025-10-30T07:46:58.167953Z","iopub.status.idle":"2025-10-30T07:46:58.190538Z","shell.execute_reply.started":"2025-10-30T07:46:58.167929Z","shell.execute_reply":"2025-10-30T07:46:58.189786Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/shodhh/accepted_2007_to_2018Q4.csv\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\n# --- Configuration ---\nprint(\"--- [New Pipeline] Part 1: Loading & Initial Setup ---\")\nDATA_FILE_PATH = '/kaggle/input/shodhh/accepted_2007_to_2018Q4.csv' # Make sure this path is correct\n\n# --- 1. Load Data ---\ntry:\n    df_new = pd.read_csv(DATA_FILE_PATH, low_memory=False)\n    # Drop rows where loan_status is missing\n    df_new = df_new.dropna(subset=[\"loan_status\"])\n    print(f\"✅ Full data loaded. Shape: {df_new.shape}\")\nexcept Exception as e:\n    print(f\"❌ Error loading data: {e}\")\n    df_new = pd.DataFrame() # Create empty DataFrame on error\n\nif not df_new.empty:\n    # --- 2. Sample Data (Replicating Notebook Step) ---\n    print(\"\\nSampling 100,000 rows...\")\n    sampled_df_new = df_new.sample(n=100000, random_state=42)\n    print(f\"Sampled data shape: {sampled_df_new.shape}\")\n\n    # --- 3. Define Target Variable (Notebook's Definition) ---\n    print(\"Defining target variable 'loan_condition_int' (notebook definition)...\")\n    bad_loan_statuses_nb = [\n        \"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\",\n        \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\"\n    ]\n    sampled_df_new['loan_condition_int'] = sampled_df_new['loan_status'].apply(\n        lambda status: 1 if status in bad_loan_statuses_nb else 0\n    ).astype(int)\n    print(\"Target variable defined.\")\n    print(\"Target distribution in sample:\")\n    print(sampled_df_new['loan_condition_int'].value_counts(normalize=True))\n\n    # --- 4. Map emp_length ---\n    print(\"\\nMapping 'emp_length' to 'emp_length_int'...\")\n    emp_length_mapping_nb = {\n        '10+ years': 10, '9 years': 9, '8 years': 8, '7 years': 7, '6 years': 6,\n        '5 years': 5, '4 years': 4, '3 years': 3, '2 years': 2, '1 year': 1,\n        '< 1 year': 0.5, 'n/a': 0\n    }\n    sampled_df_new['emp_length_int'] = sampled_df_new['emp_length'].map(emp_length_mapping_nb)\n\n    # --- 5. Map Region ---\n    print(\"Mapping 'addr_state' to 'region'...\")\n    state_to_region_nb = {\n        'CA': 'West', 'OR': 'West', 'UT': 'West', 'WA': 'West', 'CO': 'West', 'NV': 'West',\n        'AK': 'West', 'MT': 'West', 'HI': 'West', 'WY': 'West', 'ID': 'West', 'AZ': 'SouthWest',\n        'TX': 'SouthWest', 'NM': 'SouthWest', 'OK': 'SouthWest', 'GA': 'SouthEast', 'NC': 'SouthEast',\n        'VA': 'SouthEast', 'FL': 'SouthEast', 'KY': 'SouthEast', 'SC': 'SouthEast', 'LA': 'SouthEast',\n        'AL': 'SouthEast', 'WV': 'SouthEast', 'DC': 'SouthEast', 'AR': 'SouthEast', 'DE': 'SouthEast',\n        'MS': 'SouthEast', 'TN': 'SouthEast', 'IL': 'MidWest', 'MO': 'MidWest', 'MN': 'MidWest',\n        'OH': 'MidWest', 'WI': 'MidWest', 'KS': 'MidWest', 'MI': 'MidWest', 'SD': 'MidWest',\n        'IA': 'MidWest', 'NE': 'MidWest', 'IN': 'MidWest', 'ND': 'MidWest', 'CT': 'NorthEast',\n        'NY': 'NorthEast', 'PA': 'NorthEast', 'NJ': 'NorthEast', 'RI': 'NorthEast', 'MA': 'NorthEast',\n        'MD': 'NorthEast', 'VT': 'NorthEast', 'NH': 'NorthEast', 'ME': 'NorthEast'\n    }\n    sampled_df_new['region'] = sampled_df_new['addr_state'].map(state_to_region_nb)\n\n    # Store for next step\n    newpipe_step1_df = sampled_df_new\n    print(\"\\n✅ Initial setup complete.\")\n\nelse:\n    print(\"❌ Cannot proceed, data loading failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:46:58.191914Z","iopub.execute_input":"2025-10-30T07:46:58.192200Z","iopub.status.idle":"2025-10-30T07:48:16.043601Z","shell.execute_reply.started":"2025-10-30T07:46:58.192185Z","shell.execute_reply":"2025-10-30T07:48:16.042819Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 1: Loading & Initial Setup ---\n✅ Full data loaded. Shape: (2260668, 151)\n\nSampling 100,000 rows...\nSampled data shape: (100000, 151)\nDefining target variable 'loan_condition_int' (notebook definition)...\nTarget variable defined.\nTarget distribution in sample:\nloan_condition_int\n0    0.86628\n1    0.13372\nName: proportion, dtype: float64\n\nMapping 'emp_length' to 'emp_length_int'...\nMapping 'addr_state' to 'region'...\n\n✅ Initial setup complete.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming 'newpipe_step1_df' is the sampled DataFrame from the previous step\n\nprint(\"--- [New Pipeline] Part 2: Data Cleaning (Exclusions) --\")\nprint(\"--- (MODIFIED: 'loan_status' column is now KEPT) ---\")\n\n\nif 'newpipe_step1_df' in locals() or 'newpipe_step1_df' in globals():\n    df_cleaning_new = newpipe_step1_df.copy()\n    original_shape = df_cleaning_new.shape\n    print(f\"Shape before cleaning: {original_shape}\")\n\n    # --- 1. Remove 'Current' and 'Issued' loan_status --\n    print(\"\\nRemoving 'Current' and 'Issued' loan statuses...\")\n    initial_rows = len(df_cleaning_new)\n    df_cleaning_new = df_cleaning_new[~df_cleaning_new['loan_status'].isin(['Current', 'Issued'])]\n    rows_removed = initial_rows - len(df_cleaning_new)\n    print(f\"Removed {rows_removed} rows. New shape: {df_cleaning_new.shape}\")\n\n    # --- 2. Drop columns with > 80% missing values --\n    print(\"\\nDropping columns with > 80% missing values...\")\n    initial_cols = df_cleaning_new.shape[1]\n    # Keep columns with at least 20% non-missing data\n    df_cleaning_new = df_cleaning_new.dropna(axis=1, thresh=int(0.20 * len(df_cleaning_new)))\n    cols_dropped = initial_cols - df_cleaning_new.shape[1]\n    print(f\"Dropped {cols_dropped} columns. New shape: {df_cleaning_new.shape}\")\n\n    # --- 3. Drop direct indicator columns (as defined in notebook) --\n    print(\"\\nDropping direct indicator columns...\")\n    direct_indicators_nb = [\n        'collection_recovery_fee', 'last_pymnt_amnt', 'out_prncp', 'out_prncp_inv',\n        'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int',\n        'total_rec_late_fee', 'total_rec_prncp', 'next_pymnt_d'\n    ]\n    # Ensure columns exist before dropping\n    direct_indicators_to_drop = [col for col in direct_indicators_nb if col in df_cleaning_new.columns]\n    df_cleaning_new.drop(columns=direct_indicators_to_drop, inplace=True, errors='ignore')\n    print(f\"Dropped {len(direct_indicators_to_drop)} indicator columns. New shape: {df_cleaning_new.shape}\")\n\n\n    # --- 4. Drop repetitive/useless object columns (as defined in notebook) --\n    print(\"\\nDropping repetitive/useless object columns...\")\n    \n    # --- [MODIFICATION] ---\n    # 'loan_status' has been REMOVED from this list. We need it.\n    misc_cols_to_drop_nb = [\n        'emp_length', # Keeping emp_length_int\n        'id', 'emp_title', 'url', 'title', 'zip_code',\n        # 'loan_status', # <-- KEPT FOR RL REWARD\n        'addr_state' # Keep region\n    ]\n     # Ensure columns exist before dropping\n    misc_cols_to_drop = [col for col in misc_cols_to_drop_nb if col in df_cleaning_new.columns]\n    df_cleaning_new.drop(columns=misc_cols_to_drop, inplace=True, errors='ignore')\n    print(f\"Dropped {len(misc_cols_to_drop)} misc columns. New shape: {df_cleaning_new.shape}\")\n\n    # Store for next step\n    newpipe_step2_df = df_cleaning_new\n    print(\"\\n✅ Exclusion steps complete.\")\n\nelse:\n    print(\"❌ Error: 'newpipe_step1_df' not found. Please re-run Part 1.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:16.044389Z","iopub.execute_input":"2025-10-30T07:48:16.044705Z","iopub.status.idle":"2025-10-30T07:48:16.540610Z","shell.execute_reply.started":"2025-10-30T07:48:16.044686Z","shell.execute_reply":"2025-10-30T07:48:16.539728Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 2: Data Cleaning (Exclusions) --\n--- (MODIFIED: 'loan_status' column is now KEPT) ---\nShape before cleaning: (100000, 154)\n\nRemoving 'Current' and 'Issued' loan statuses...\nRemoved 38822 rows. New shape: (61178, 154)\n\nDropping columns with > 80% missing values...\nDropped 40 columns. New shape: (61178, 114)\n\nDropping direct indicator columns...\nDropped 10 indicator columns. New shape: (61178, 104)\n\nDropping repetitive/useless object columns...\nDropped 7 misc columns. New shape: (61178, 97)\n\n✅ Exclusion steps complete.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming 'newpipe_step2_df' is the DataFrame after the exclusion steps\n\nprint(\"--- [New Pipeline] Part 3: Missing Value Imputation ---\")\n\nif 'newpipe_step2_df' in locals() or 'newpipe_step2_df' in globals():\n    fillna_df_new = newpipe_step2_df.copy()\n    print(f\"Shape before imputation: {fillna_df_new.shape}\")\n    print(f\"Total missing values before: {fillna_df_new.isnull().sum().sum()}\")\n\n    # --- Impute Object Columns (Mode by Region) ---\n    print(\"\\nImputing object columns by region mode...\")\n    object_cols_to_impute = [\"last_pymnt_d\", \"last_credit_pull_d\"]\n    for column in object_cols_to_impute:\n        if column in fillna_df_new.columns:\n            # Calculate mode for each region\n            mode_map = fillna_df_new.groupby(\"region\")[column].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n            # Fill NaNs using the map\n            fillna_df_new[column] = fillna_df_new.apply(lambda row: mode_map[row['region']] if pd.isnull(row[column]) else row[column], axis=1)\n            # Fallback for any regions that might have had only NaNs (fill with overall mode)\n            overall_mode = fillna_df_new[column].mode()[0] if not fillna_df_new[column].mode().empty else 'Unknown'\n            fillna_df_new[column].fillna(overall_mode, inplace=True)\n\n\n    # --- Impute Numerical Columns (Median by Region) ---\n    print(\"Imputing specific numerical columns by region median...\")\n    median_cols_to_impute = [\"pub_rec\", \"total_acc\", \"emp_length_int\"]\n    for column in median_cols_to_impute:\n        if column in fillna_df_new.columns:\n            fillna_df_new[column] = fillna_df_new.groupby(\"region\")[column].transform(lambda x: x.fillna(x.median()))\n            # Fallback for any remaining NaNs (e.g., if a whole region was NaN)\n            fillna_df_new[column].fillna(fillna_df_new[column].median(), inplace=True)\n\n\n    # --- Impute Numerical Columns (Mean by Region) ---\n    print(\"Imputing specific numerical columns by region mean...\")\n    mean_cols_to_impute = [\"annual_inc\", \"delinq_2yrs\"]\n    for column in mean_cols_to_impute:\n         if column in fillna_df_new.columns:\n            fillna_df_new[column] = fillna_df_new.groupby(\"region\")[column].transform(lambda x: x.fillna(x.mean()))\n            # Fallback for any remaining NaNs\n            fillna_df_new[column].fillna(fillna_df_new[column].mean(), inplace=True)\n\n    # --- Fill Remaining NaNs with Zero (as per notebook) ---\n    print(\"Filling all remaining NaNs with 0...\")\n    initial_nan_count = fillna_df_new.isnull().sum().sum()\n    fillna_df_new.fillna(0, inplace=True)\n    final_nan_count = fillna_df_new.isnull().sum().sum()\n    print(f\"Filled {initial_nan_count - final_nan_count} remaining NaN values.\")\n\n    # Store for next step\n    newpipe_step3_df = fillna_df_new\n    print(f\"\\n✅ Imputation complete. Final shape: {newpipe_step3_df.shape}\")\n    print(f\"Total missing values after: {newpipe_step3_df.isnull().sum().sum()}\")\n\nelse:\n    print(\"❌ Error: 'newpipe_step2_df' not found. Please re-run Part 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:16.542286Z","iopub.execute_input":"2025-10-30T07:48:16.542508Z","iopub.status.idle":"2025-10-30T07:48:18.290444Z","shell.execute_reply.started":"2025-10-30T07:48:16.542492Z","shell.execute_reply":"2025-10-30T07:48:18.289576Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 3: Missing Value Imputation ---\nShape before imputation: (61178, 97)\nTotal missing values before: 784318\n\nImputing object columns by region mode...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_143/328690479.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(overall_mode, inplace=True)\n/tmp/ipykernel_143/328690479.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(overall_mode, inplace=True)\n/tmp/ipykernel_143/328690479.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(fillna_df_new[column].median(), inplace=True)\n/tmp/ipykernel_143/328690479.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(fillna_df_new[column].median(), inplace=True)\n/tmp/ipykernel_143/328690479.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(fillna_df_new[column].median(), inplace=True)\n/tmp/ipykernel_143/328690479.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(fillna_df_new[column].mean(), inplace=True)\n/tmp/ipykernel_143/328690479.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_new[column].fillna(fillna_df_new[column].mean(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Imputing specific numerical columns by region median...\nImputing specific numerical columns by region mean...\nFilling all remaining NaNs with 0...\nFilled 780549 remaining NaN values.\n\n✅ Imputation complete. Final shape: (61178, 97)\nTotal missing values after: 0\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming 'newpipe_step3_df' is the DataFrame after imputation\n\nprint(\"--- [New Pipeline] Part 4: Removing Outliers ---\")\n\nif 'newpipe_step3_df' in locals() or 'newpipe_step3_df' in globals():\n    RemoveOutlier_df_new = newpipe_step3_df.copy()\n    print(f\"Shape before removing outliers: {RemoveOutlier_df_new.shape}\")\n\n    # Apply custom thresholds as used in the notebook\n    initial_rows = len(RemoveOutlier_df_new)\n\n    if 'annual_inc' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['annual_inc'] <= 250000]\n    if 'dti' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['dti'] <= 50]\n    if 'open_acc' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['open_acc'] <= 40]\n    if 'total_acc' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['total_acc'] <= 80]\n    if 'revol_util' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['revol_util'] <= 120]\n    if 'revol_bal' in RemoveOutlier_df_new.columns:\n        RemoveOutlier_df_new = RemoveOutlier_df_new[RemoveOutlier_df_new['revol_bal'] <= 250000]\n\n    # Reset index after filtering\n    RemoveOutlier_df_new.reset_index(drop=True, inplace=True)\n\n    rows_removed = initial_rows - len(RemoveOutlier_df_new)\n    print(f\"Removed {rows_removed} rows due to outlier thresholds.\")\n    print(f\"Shape after removing outliers: {RemoveOutlier_df_new.shape}\")\n\n    # Store for next step\n    newpipe_step4_df = RemoveOutlier_df_new\n    print(\"\\n✅ Outlier removal complete.\")\n\nelse:\n    print(\"❌ Error: 'newpipe_step3_df' not found. Please re-run Part 3.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:18.291278Z","iopub.execute_input":"2025-10-30T07:48:18.291525Z","iopub.status.idle":"2025-10-30T07:48:18.435220Z","shell.execute_reply.started":"2025-10-30T07:48:18.291496Z","shell.execute_reply":"2025-10-30T07:48:18.434272Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 4: Removing Outliers ---\nShape before removing outliers: (61178, 97)\nRemoved 925 rows due to outlier thresholds.\nShape after removing outliers: (60253, 97)\n\n✅ Outlier removal complete.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Ensure category_encoders is installed\ntry:\n    from category_encoders import TargetEncoder\n    print(\"✅ category_encoders imported successfully.\")\nexcept ImportError:\n    print(\"Warning: category_encoders not found. Attempting install...\")\n    try:\n        import sys\n        # Need to ensure scikit-learn is compatible first, as per previous errors\n        !{sys.executable} -m pip install scikit-learn==1.5.2 --force-reinstall --quiet\n        !{sys.executable} -m pip install category_encoders --quiet\n        from category_encoders import TargetEncoder\n        print(\"✅ Installation successful.\")\n    except Exception as e:\n        print(f\"❌ Error installing category_encoders: {e}\")\n        TargetEncoder = None\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming 'newpipe_step4_df' is the DataFrame after outlier removal\n# Assuming 'loan_condition_int' is the target column name\n\nprint(\"--- [New Pipeline] Part 5: Feature Engineering ---\")\n\nif 'newpipe_step4_df' in locals() or 'newpipe_step4_df' in globals():\n    if TargetEncoder is None:\n         print(\"❌ Cannot proceed, TargetEncoder failed to import or install.\")\n    else:\n        FE_df_new = newpipe_step4_df.copy()\n        target_col_nb = 'loan_condition_int'\n\n        # --- 1. Identify Feature Types ---\n        original_cols_fe = FE_df_new.columns.tolist()\n        cat_cols_fe = FE_df_new.select_dtypes(include=['object']).columns.tolist()\n        # Exclude the target variable from numerical columns\n        num_cols_fe = FE_df_new.select_dtypes(exclude=['object']).columns.drop(target_col_nb, errors='ignore').tolist()\n\n        # Separate categorical into binary and multi-category\n        dual_cat_cols_fe = [col for col in cat_cols_fe if FE_df_new[col].nunique() <= 2]\n        multi_cat_cols_fe = [col for col in cat_cols_fe if FE_df_new[col].nunique() > 2]\n\n        print(f\"Numerical columns found: {len(num_cols_fe)}\")\n        print(f\"Binary categorical columns: {dual_cat_cols_fe}\")\n        print(f\"Multi-categorical columns: {multi_cat_cols_fe}\")\n\n        # --- 2. Binary Encoding (get_dummies) ---\n        print(\"\\nApplying Binary Encoding (get_dummies)...\")\n        FE_df_new = pd.get_dummies(FE_df_new, columns=dual_cat_cols_fe, drop_first=True)\n        # Get names of newly created binary columns\n        new_binary_cols = [col for col in FE_df_new.columns if col not in original_cols_fe and col != target_col_nb]\n        print(f\"Created {len(new_binary_cols)} new binary columns.\")\n\n        # --- 3. Train/Test Split (Stratified) ---\n        print(\"\\nSplitting data into training (80%) and test (20%) sets...\")\n        stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n        # Perform the split\n        for train_idx, test_idx in stratified_split.split(FE_df_new, FE_df_new[target_col_nb]):\n            train_df_nb = FE_df_new.loc[train_idx]\n            test_df_nb = FE_df_new.loc[test_idx]\n\n        # Separate features and target\n        train_y_nb = train_df_nb[[target_col_nb]]\n        test_y_nb = test_df_nb[[target_col_nb]]\n        train_X_nb = train_df_nb.drop(target_col_nb, axis=1)\n        test_X_nb = test_df_nb.drop(target_col_nb, axis=1)\n\n        print(f\"Training set shape: X={train_X_nb.shape}, y={train_y_nb.shape}\")\n        print(f\"Test set shape:     X={test_X_nb.shape}, y={test_y_nb.shape}\")\n\n        # --- 4. Target Encoding ---\n        print(\"\\nApplying Target Encoding...\")\n        # Ensure only existing multi-cat columns are processed\n        multi_cat_cols_to_encode = [col for col in multi_cat_cols_fe if col in train_X_nb.columns]\n        target_encoder_nb = TargetEncoder(cols=multi_cat_cols_to_encode, smoothing=0.2) # Notebook used smoothing=0.2\n\n        # Fit ONLY on training data\n        target_encoder_nb.fit(train_X_nb, train_y_nb.values.ravel()) # .values.ravel() converts to 1D array\n\n        # Transform both train and test data\n        train_X_encoded = target_encoder_nb.transform(train_X_nb)\n        test_X_encoded = target_encoder_nb.transform(test_X_nb)\n        print(\"Target Encoding applied.\")\n        # Store list of newly numerical columns from target encoding\n        target_encoded_numeric_cols = multi_cat_cols_to_encode\n\n        # --- 5. Normalization (StandardScaler) ---\n        print(\"\\nApplying Normalization (StandardScaler)...\")\n        scaler_nb = StandardScaler()\n\n        # Identify all numerical columns for scaling (original + target encoded + new binary)\n        cols_to_scale = num_cols_fe + target_encoded_numeric_cols + new_binary_cols\n        # Filter out any columns that might have been dropped or don't exist\n        cols_to_scale = [col for col in cols_to_scale if col in train_X_encoded.columns]\n\n\n        # Fit ONLY on training data\n        print(f\"Fitting scaler on {len(cols_to_scale)} numerical features...\")\n        scaler_nb.fit(train_X_encoded[cols_to_scale])\n\n        # Transform both train and test data (in place)\n        train_X_scaled = train_X_encoded.copy()\n        test_X_scaled = test_X_encoded.copy()\n\n        train_X_scaled[cols_to_scale] = scaler_nb.transform(train_X_encoded[cols_to_scale])\n        test_X_scaled[cols_to_scale] = scaler_nb.transform(test_X_encoded[cols_to_scale])\n        print(\"Normalization applied.\")\n\n        # Store final datasets for next step\n        newpipe_step5_train_X = train_X_scaled\n        newpipe_step5_train_y = train_y_nb\n        newpipe_step5_test_X = test_X_scaled\n        newpipe_step5_test_y = test_y_nb\n\n        print(f\"\\n✅ Feature Engineering complete.\")\n        print(f\"Final training X shape: {newpipe_step5_train_X.shape}\")\n        print(f\"Final test X shape: {newpipe_step5_test_X.shape}\")\n\nelse:\n    print(\"❌ Error: 'newpipe_step4_df' not found. Please re-run Part 4.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:18.436218Z","iopub.execute_input":"2025-10-30T07:48:18.436605Z","iopub.status.idle":"2025-10-30T07:48:19.690020Z","shell.execute_reply.started":"2025-10-30T07:48:18.436575Z","shell.execute_reply":"2025-10-30T07:48:19.689183Z"}},"outputs":[{"name":"stdout","text":"✅ category_encoders imported successfully.\n--- [New Pipeline] Part 5: Feature Engineering ---\nNumerical columns found: 78\nBinary categorical columns: ['term', 'pymnt_plan', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']\nMulti-categorical columns: ['grade', 'sub_grade', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'purpose', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d', 'region']\n\nApplying Binary Encoding (get_dummies)...\nCreated 7 new binary columns.\n\nSplitting data into training (80%) and test (20%) sets...\nTraining set shape: X=(48202, 96), y=(48202, 1)\nTest set shape:     X=(12051, 96), y=(12051, 1)\n\nApplying Target Encoding...\nTarget Encoding applied.\n\nApplying Normalization (StandardScaler)...\nFitting scaler on 96 numerical features...\nNormalization applied.\n\n✅ Feature Engineering complete.\nFinal training X shape: (48202, 96)\nFinal test X shape: (12051, 96)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Install compatible scikit-learn\n!pip install scikit-learn==1.5.2 --force-reinstall --quiet\n# Install imbalanced-learn\n!pip install imbalanced-learn --quiet\n\nprint(\"✅ Installation block complete. Please RESTART YOUR KERNEL now.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:19.690809Z","iopub.execute_input":"2025-10-30T07:48:19.691049Z","iopub.status.idle":"2025-10-30T07:48:39.390895Z","shell.execute_reply.started":"2025-10-30T07:48:19.691030Z","shell.execute_reply":"2025-10-30T07:48:39.390051Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ Installation block complete. Please RESTART YOUR KERNEL now.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# We will try to import *after* the kernel restart\ntry:\n    from imblearn.under_sampling import RandomUnderSampler\n    print(\"✅ imblearn imported successfully.\")\n    _imblearn_installed = True\nexcept ImportError:\n    print(\"❌ Error: imbalanced-learn still not found or import failed.\")\n    print(\"Please ensure you RESTARTED THE KERNEL after running the install block.\")\n    _imblearn_installed = False\nexcept Exception as e:\n    print(f\"❌ An unexpected error occurred during import: {e}\")\n    _imblearn_installed = False\n\n# --- Setup ---\n# Assuming newpipe_step5_train_X, newpipe_step5_train_y, etc. exist\n\nprint(\"--- [New Pipeline] Part 6: Applying Random Undersampling ---\")\n\n# Check if inputs exist and RandomUnderSampler is available\nif _imblearn_installed:\n    if 'newpipe_step5_train_X' not in locals() or 'newpipe_step5_train_y' not in locals():\n        print(\"❌ Error: Input training data not found. Please ensure Steps 1-5 were rerun successfully after restarting.\")\n    else:\n        # --- 1. Initialize Undersampler ---\n        rus_nb = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n\n        # --- 2. Apply Undersampling ONLY to Training Data ---\n        print(f\"Original training data shape: X={newpipe_step5_train_X.shape}, y={newpipe_step5_train_y.shape}\")\n        print(\"Original training target distribution:\")\n        print(newpipe_step5_train_y['loan_condition_int'].value_counts()) # Access column in DataFrame\n\n        try:\n            # Pass DataFrame/Series directly\n            X_train_undersampled_nb, y_train_undersampled_nb = rus_nb.fit_resample(\n                newpipe_step5_train_X, newpipe_step5_train_y['loan_condition_int'] # Pass Series\n            )\n\n            print(f\"\\nUndersampled training data shape: X={X_train_undersampled_nb.shape}, y={y_train_undersampled_nb.shape}\")\n            print(\"Undersampled training target distribution:\")\n            print(y_train_undersampled_nb.value_counts()) # Now it's a Series\n\n            # --- Store Final Datasets for Modeling ---\n            newpipe_step6_train_X = X_train_undersampled_nb\n            newpipe_step6_train_y = y_train_undersampled_nb # This is now a Series\n            newpipe_step6_test_X = newpipe_step5_test_X\n            newpipe_step6_test_y = newpipe_step5_test_y\n\n            print(\"\\n✅ Undersampling complete. Datasets ready for feature selection.\")\n\n        except Exception as e:\n            print(f\"❌ An error occurred during fit_resample: {e}\")\nelse:\n    print(\"❌ Cannot proceed because imblearn failed to import.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:39.392434Z","iopub.execute_input":"2025-10-30T07:48:39.392737Z","iopub.status.idle":"2025-10-30T07:48:39.460854Z","shell.execute_reply.started":"2025-10-30T07:48:39.392713Z","shell.execute_reply":"2025-10-30T07:48:39.460197Z"}},"outputs":[{"name":"stdout","text":"✅ imblearn imported successfully.\n--- [New Pipeline] Part 6: Applying Random Undersampling ---\nOriginal training data shape: X=(48202, 96), y=(48202, 1)\nOriginal training target distribution:\nloan_condition_int\n0    37664\n1    10538\nName: count, dtype: int64\n\nUndersampled training data shape: X=(21076, 96), y=(21076,)\nUndersampled training target distribution:\nloan_condition_int\n0    10538\n1    10538\nName: count, dtype: int64\n\n✅ Undersampling complete. Datasets ready for feature selection.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import VarianceThreshold\n\n# --- Setup ---\n# Assuming newpipe_step6_train_X (undersampled, scaled, encoded training features) exists\n# Assuming newpipe_step6_test_X (original scaled, encoded test features) exists\n\nprint(\"--- [New Pipeline] Part 7: Feature Selection ---\")\n\nif 'newpipe_step6_train_X' not in locals() or 'newpipe_step6_test_X' not in locals():\n    print(\"❌ Error: Input data (newpipe_step6_train_X or newpipe_step6_test_X) not found.\")\n    print(\"Please ensure Step 6 (Undersampling) completed successfully.\")\nelse:\n    X_train_fs = newpipe_step6_train_X.copy()\n    X_test_fs = newpipe_step6_test_X.copy() # Apply selection to test set too\n    # The notebook implicitly uses the undersampled y_train for wrapper, but we only need X for VarianceThreshold\n    # y_train_fs = newpipe_step6_train_y # Undersampled training target\n\n    print(f\"Shape before VarianceThreshold: {X_train_fs.shape}\")\n\n    # --- 1. Apply VarianceThreshold ---\n    # The notebook used threshold=1 on the *scaled* data\n    selector = VarianceThreshold(threshold=1)\n    # Fit on the training data\n    selector.fit(X_train_fs)\n\n    # Get the names of the features kept by the threshold\n    filtered_feature_names = X_train_fs.columns[selector.get_support()]\n    # Apply filter to train X\n    X_train_variance_filtered = X_train_fs[filtered_feature_names]\n    # Apply the same filter to test X\n    X_test_variance_filtered = X_test_fs[filtered_feature_names]\n\n    cols_removed = X_train_fs.shape[1] - X_train_variance_filtered.shape[1]\n    print(f\"Applied VarianceThreshold(1). Removed {cols_removed} features.\")\n    print(f\"Shape after VarianceThreshold: {X_train_variance_filtered.shape}\")\n\n    # --- 2. Select Final Features (Based on Notebook's Wrapper Result) ---\n    # The notebook ran a time-consuming wrapper (SFS) and identified these 9 features.\n    # We will directly select these for replication purposes.\n    vars_final_nb = [\n        'delinq_2yrs',\n        'last_fico_range_high',\n        'last_fico_range_low',\n        'acc_now_delinq',\n        'open_acc_6m',\n        'total_bal_il',\n        'il_util',\n        'open_rv_12m',\n        'all_util'\n     ]\n    print(f\"\\nSelecting the final {len(vars_final_nb)} features identified by the notebook's wrapper method...\")\n\n    # Ensure these final columns actually exist after variance thresholding\n    final_cols_exist = [col for col in vars_final_nb if col in X_train_variance_filtered.columns]\n\n    if len(final_cols_exist) != len(vars_final_nb):\n        print(f\"⚠️ Warning: Not all expected final features ({vars_final_nb}) were present after VarianceThreshold.\")\n        print(f\"Features missing: {list(set(vars_final_nb) - set(final_cols_exist))}\")\n        print(f\"Proceeding with the {len(final_cols_exist)} available features: {final_cols_exist}\")\n        final_selected_cols = final_cols_exist\n    else:\n        final_selected_cols = vars_final_nb\n        print(\"All expected final features found.\")\n\n\n    # Apply the final selection to both train and test sets\n    X_train_final_selected = X_train_variance_filtered[final_selected_cols]\n    X_test_final_selected = X_test_variance_filtered[final_selected_cols]\n\n\n    # Store final datasets for modeling\n    newpipe_step7_train_X = X_train_final_selected\n    newpipe_step7_train_y = newpipe_step6_train_y # Use the undersampled y from step 6\n    newpipe_step7_test_X = X_test_final_selected\n    newpipe_step7_test_y = newpipe_step6_test_y # Use the original test y from step 6\n\n    print(f\"\\n✅ Feature Selection complete.\")\n    print(f\"Final training X shape: {newpipe_step7_train_X.shape}\")\n    print(f\"Final test X shape: {newpipe_step7_test_X.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:39.463028Z","iopub.execute_input":"2025-10-30T07:48:39.463508Z","iopub.status.idle":"2025-10-30T07:48:39.516718Z","shell.execute_reply.started":"2025-10-30T07:48:39.463489Z","shell.execute_reply":"2025-10-30T07:48:39.516008Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 7: Feature Selection ---\nShape before VarianceThreshold: (21076, 96)\nApplied VarianceThreshold(1). Removed 37 features.\nShape after VarianceThreshold: (21076, 59)\n\nSelecting the final 9 features identified by the notebook's wrapper method...\nAll expected final features found.\n\n✅ Feature Selection complete.\nFinal training X shape: (21076, 9)\nFinal test X shape: (12051, 9)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# --- [New Pipeline] Part 8: Task 2 - DL Predictive Model ---\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom sklearn.metrics import f1_score, roc_auc_score, classification_report\nimport numpy as np\n\nprint(\"--- [New Pipeline] Part 8: Task 2 - DL Predictive Model ---\")\n\n# --- 1. Load Data (from your previous step) ---\nX_train_dl = newpipe_step7_train_X\ny_train_dl = newpipe_step7_train_y\nX_test_dl = newpipe_step7_test_X\ny_test_dl = newpipe_step7_test_y\n\nprint(f\"DL Model Input Shapes: X_train={X_train_dl.shape}, y_train={y_train_dl.shape}\")\nprint(f\"DL Model Test Shapes:  X_test={X_test_dl.shape}, y_test={y_test_dl.shape}\")\n\n# --- 2. Define the DL Model (MLP) ---\n# We have 9 input features\nn_features = X_train_dl.shape[1]\n\ndl_model = Sequential([\n    Dense(64, activation='relu', input_shape=(n_features,)),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid') # Binary output\n])\n\ndl_model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        AUC(name='auc'),\n        Precision(name='precision'),\n        Recall(name='recall')\n    ]\n)\n\ndl_model.summary()\n\n# --- 3. Train the Model ---\nprint(\"\\nTraining the DL model...\")\nhistory = dl_model.fit(\n    X_train_dl,\n    y_train_dl,\n    epochs=20, # You can tune this\n    batch_size=128,\n    validation_split=0.2, # Use part of training data for validation\n    verbose=1\n)\n\n# --- 4. Evaluate the Model (as per assignment) ---\nprint(\"\\nEvaluating model on the hold-out test set...\")\n# Get predicted probabilities\ny_pred_proba_dl = dl_model.predict(X_test_dl).ravel()\n# Get predicted classes (using 0.5 threshold)\ny_pred_class_dl = (y_pred_proba_dl > 0.5).astype(int)\n\n# Calculate required metrics [cite: 33]\nauc_score = roc_auc_score(y_test_dl, y_pred_proba_dl)\nf1 = f1_score(y_test_dl, y_pred_class_dl)\n\nprint(\"\\n--- Task 2 Evaluation Metrics ---\")\nprint(f\"✅ AUC (Area Under the ROC Curve): {auc_score:.4f}\")\nprint(f\"✅ F1-Score: {f1:.4f}\")\n\nprint(\"\\nFull Classification Report on Test Set:\")\nprint(classification_report(y_test_dl, y_pred_class_dl, target_names=['Fully Paid (0)', 'Defaulted (1)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T07:48:39.517622Z","iopub.execute_input":"2025-10-30T07:48:39.517908Z","iopub.status.idle":"2025-10-30T07:48:58.058219Z","shell.execute_reply.started":"2025-10-30T07:48:39.517888Z","shell.execute_reply":"2025-10-30T07:48:58.057497Z"}},"outputs":[{"name":"stdout","text":"--- [New Pipeline] Part 8: Task 2 - DL Predictive Model ---\nDL Model Input Shapes: X_train=(21076, 9), y_train=(21076,)\nDL Model Test Shapes:  X_test=(12051, 9), y_test=(12051, 1)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,753\u001b[0m (10.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,753</span> (10.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,753\u001b[0m (10.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,753</span> (10.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nTraining the DL model...\nEpoch 1/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - auc: 0.7197 - loss: 0.5903 - precision: 0.6041 - recall: 0.5857 - val_auc: 0.0000e+00 - val_loss: 0.4622 - val_precision: 1.0000 - val_recall: 0.8067\nEpoch 2/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9196 - loss: 0.3533 - precision: 0.8279 - recall: 0.8176 - val_auc: 0.0000e+00 - val_loss: 0.4475 - val_precision: 1.0000 - val_recall: 0.8430\nEpoch 3/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9295 - loss: 0.3297 - precision: 0.8224 - recall: 0.8425 - val_auc: 0.0000e+00 - val_loss: 0.4922 - val_precision: 1.0000 - val_recall: 0.8283\nEpoch 4/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9242 - loss: 0.3386 - precision: 0.8130 - recall: 0.8411 - val_auc: 0.0000e+00 - val_loss: 0.4843 - val_precision: 1.0000 - val_recall: 0.8349\nEpoch 5/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9256 - loss: 0.3337 - precision: 0.8189 - recall: 0.8435 - val_auc: 0.0000e+00 - val_loss: 0.4471 - val_precision: 1.0000 - val_recall: 0.8418\nEpoch 6/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9271 - loss: 0.3329 - precision: 0.8154 - recall: 0.8451 - val_auc: 0.0000e+00 - val_loss: 0.4632 - val_precision: 1.0000 - val_recall: 0.8423\nEpoch 7/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9286 - loss: 0.3265 - precision: 0.8169 - recall: 0.8487 - val_auc: 0.0000e+00 - val_loss: 0.4734 - val_precision: 1.0000 - val_recall: 0.8382\nEpoch 8/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9266 - loss: 0.3310 - precision: 0.8174 - recall: 0.8510 - val_auc: 0.0000e+00 - val_loss: 0.4861 - val_precision: 1.0000 - val_recall: 0.8254\nEpoch 9/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - auc: 0.9271 - loss: 0.3301 - precision: 0.8146 - recall: 0.8394 - val_auc: 0.0000e+00 - val_loss: 0.4779 - val_precision: 1.0000 - val_recall: 0.8280\nEpoch 10/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9328 - loss: 0.3173 - precision: 0.8197 - recall: 0.8584 - val_auc: 0.0000e+00 - val_loss: 0.4655 - val_precision: 1.0000 - val_recall: 0.8389\nEpoch 11/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9294 - loss: 0.3254 - precision: 0.8163 - recall: 0.8521 - val_auc: 0.0000e+00 - val_loss: 0.4816 - val_precision: 1.0000 - val_recall: 0.8366\nEpoch 12/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9300 - loss: 0.3239 - precision: 0.8206 - recall: 0.8458 - val_auc: 0.0000e+00 - val_loss: 0.4902 - val_precision: 1.0000 - val_recall: 0.8264\nEpoch 13/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - auc: 0.9253 - loss: 0.3334 - precision: 0.8143 - recall: 0.8451 - val_auc: 0.0000e+00 - val_loss: 0.4433 - val_precision: 1.0000 - val_recall: 0.8501\nEpoch 14/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - auc: 0.9324 - loss: 0.3198 - precision: 0.8204 - recall: 0.8531 - val_auc: 0.0000e+00 - val_loss: 0.4548 - val_precision: 1.0000 - val_recall: 0.8399\nEpoch 15/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9289 - loss: 0.3261 - precision: 0.8142 - recall: 0.8479 - val_auc: 0.0000e+00 - val_loss: 0.4960 - val_precision: 1.0000 - val_recall: 0.8340\nEpoch 16/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9296 - loss: 0.3243 - precision: 0.8225 - recall: 0.8557 - val_auc: 0.0000e+00 - val_loss: 0.4974 - val_precision: 1.0000 - val_recall: 0.8352\nEpoch 17/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9330 - loss: 0.3178 - precision: 0.8228 - recall: 0.8525 - val_auc: 0.0000e+00 - val_loss: 0.4699 - val_precision: 1.0000 - val_recall: 0.8356\nEpoch 18/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9300 - loss: 0.3234 - precision: 0.8199 - recall: 0.8538 - val_auc: 0.0000e+00 - val_loss: 0.4542 - val_precision: 1.0000 - val_recall: 0.8416\nEpoch 19/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc: 0.9262 - loss: 0.3318 - precision: 0.8090 - recall: 0.8476 - val_auc: 0.0000e+00 - val_loss: 0.4646 - val_precision: 1.0000 - val_recall: 0.8352\nEpoch 20/20\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - auc: 0.9311 - loss: 0.3212 - precision: 0.8242 - recall: 0.8483 - val_auc: 0.0000e+00 - val_loss: 0.4551 - val_precision: 1.0000 - val_recall: 0.8413\n\nEvaluating model on the hold-out test set...\n\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n\n--- Task 2 Evaluation Metrics ---\n✅ AUC (Area Under the ROC Curve): 0.9317\n✅ F1-Score: 0.7535\n\nFull Classification Report on Test Set:\n                precision    recall  f1-score   support\n\nFully Paid (0)       0.95      0.89      0.92      9417\n Defaulted (1)       0.68      0.85      0.75      2634\n\n      accuracy                           0.88     12051\n     macro avg       0.82      0.87      0.84     12051\n  weighted avg       0.89      0.88      0.88     12051\n\n","output_type":"stream"}],"execution_count":31}]}