{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13553238,"sourceType":"datasetVersion","datasetId":8608151},{"sourceId":13546623,"sourceType":"datasetVersion","datasetId":8603326}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Configuration\n# EITHER: point to Task-1 engineered file (recommended: faster, consistent)\nUSE_CLEAN_FIRST = True\nCLEAN_PATH = \"loan_clean_subset.csv\"   # upload this if you have it\n\n# OR: fall back to raw file (we'll do a minimal preprocess if clean is absent)\nRAW_PATH = \"/kaggle/input/shodhh/accepted_2007_to_2018Q4.csv\"  # upload if needed\nNROWS_FROM_RAW = 200_000    # None for full data (Colab can handle; start smaller if RAM is tight)\n\nRANDOM_STATE = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:49:36.629287Z","iopub.execute_input":"2025-10-30T09:49:36.629576Z","iopub.status.idle":"2025-10-30T09:49:36.634443Z","shell.execute_reply.started":"2025-10-30T09:49:36.629545Z","shell.execute_reply":"2025-10-30T09:49:36.633519Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Install Libraries and Imports\n!pip install -U d3rlpy==2.4.0\n!pip install -U gymnasium[classic-control]==0.29.1\n\nimport os, gc, textwrap, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\n\nimport d3rlpy\nfrom d3rlpy.dataset import MDPDataset\nfrom d3rlpy.algos import CQL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:49:47.859003Z","iopub.execute_input":"2025-10-30T09:49:47.859823Z","iopub.status.idle":"2025-10-30T09:51:29.012738Z","shell.execute_reply.started":"2025-10-30T09:49:47.859786Z","shell.execute_reply":"2025-10-30T09:51:29.011706Z"}},"outputs":[{"name":"stdout","text":"Collecting d3rlpy==2.4.0\n  Downloading d3rlpy-2.4.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (4.67.1)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (3.14.0)\nCollecting gym>=0.26.0 (from d3rlpy==2.4.0)\n  Downloading gym-0.26.2.tar.gz (721 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (8.3.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (4.15.0)\nCollecting structlog (from d3rlpy==2.4.0)\n  Downloading structlog-25.5.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (0.4.6)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (0.6.7)\nRequirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.4.0) (0.29.0)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.4.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.4.0) (3.1.1)\nRequirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.4.0) (0.0.8)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (3.19.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->d3rlpy==2.4.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->d3rlpy==2.4.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->d3rlpy==2.4.0) (1.3.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->d3rlpy==2.4.0) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->d3rlpy==2.4.0) (0.9.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->d3rlpy==2.4.0) (0.0.4)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->d3rlpy==2.4.0) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->d3rlpy==2.4.0) (1.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->d3rlpy==2.4.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.4.0) (2024.2.0)\nDownloading d3rlpy-2.4.0-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading structlog-25.5.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827728 sha256=6cd27ee6d10ce91aede2e904f3eec61d1475d8234633de116110f3b507681432\n  Stored in directory: /root/.cache/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\nSuccessfully built gym\nInstalling collected packages: structlog, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, gym, d3rlpy\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: gym\n    Found existing installation: gym 0.25.2\n    Uninstalling gym-0.25.2:\n      Successfully uninstalled gym-0.25.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed d3rlpy-2.4.0 gym-0.26.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 structlog-25.5.0\nCollecting gymnasium==0.29.1 (from gymnasium[classic-control]==0.29.1)\n  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (3.1.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (4.15.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (0.0.4)\nRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[classic-control]==0.29.1) (2.6.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium==0.29.1->gymnasium[classic-control]==0.29.1) (2024.2.0)\nDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: gymnasium\n  Attempting uninstall: gymnasium\n    Found existing installation: gymnasium 0.29.0\n    Uninstalling gymnasium-0.29.0:\n      Successfully uninstalled gymnasium-0.29.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 0.29.1 which is incompatible.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gymnasium-0.29.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Helper Functions\nDEFAULT_LIKE = {\n    \"Charged Off\",\"Default\",\n    \"Late (31-120 days)\",\"Late (16-30 days)\",\n    \"Does not meet the credit policy. Status:Charged Off\"\n}\nPAID_LIKE = {\"Fully Paid\",\"Does not meet the credit policy. Status:Fully Paid\"}\n\ndef map_target(status: str):\n    if pd.isna(status): return np.nan\n    s = str(status).strip()\n    if s in PAID_LIKE: return 0\n    if s in DEFAULT_LIKE: return 1\n    return np.nan\n\ndef parse_pct(series: pd.Series) -> pd.Series:\n    return pd.to_numeric(series.astype(str).str.rstrip(\"%\"), errors=\"coerce\")\n\ndef parse_emp_length(series: pd.Series) -> pd.Series:\n    def _map(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"n/a\",\"na\",\"none\"): return np.nan\n        if s.startswith(\"<\"): return 0.5\n        if \"10+\" in s: return 10.0\n        for tok in s.split():\n            try: return float(tok)\n            except: pass\n        return np.nan\n    return series.apply(_map)\n\nFEATURES_RAW = [\n    \"loan_amnt\",\"funded_amnt\",\"term\",\"installment\",\"int_rate\",\n    \"annual_inc\",\"dti\",\"emp_length\",\"home_ownership\",\"verification_status\",\n    \"purpose\",\"addr_state\",\"revol_bal\",\"revol_util\",\n    \"open_acc\",\"total_acc\",\"delinq_2yrs\",\"inq_last_6mths\",\"pub_rec\",\n    \"issue_d\",\"earliest_cr_line\",\"loan_status\"\n]\n\ndef add_date_features(frame: pd.DataFrame) -> pd.DataFrame:\n    X = frame.copy()\n    if \"issue_d\" in X:\n        X[\"issue_year\"] = X[\"issue_d\"].dt.year\n        X[\"issue_month\"] = X[\"issue_d\"].dt.month\n        X[\"issue_ym\"] = X[\"issue_year\"]*12 + X[\"issue_month\"]\n    if \"earliest_cr_line\" in X:\n        if \"issue_d\" in X:\n            X[\"credit_hist_months\"] = (\n                (X[\"issue_d\"].dt.year - X[\"earliest_cr_line\"].dt.year)*12 +\n                (X[\"issue_d\"].dt.month - X[\"earliest_cr_line\"].dt.month)\n            )\n        else:\n            ref = pd.Timestamp(\"2018-12-01\")\n            X[\"credit_hist_months\"] = (\n                (ref.year - X[\"earliest_cr_line\"].dt.year)*12 +\n                (ref.month - X[\"earliest_cr_line\"].dt.month)\n            )\n    for d in (\"issue_d\",\"earliest_cr_line\"):\n        if d in X: X.drop(columns=[d], inplace=True)\n    return X\n\ndef robust_time_split(mdf: pd.DataFrame):\n    # 80/20 by issue_d if feasible; else stratified random\n    if \"issue_d\" in mdf.columns and mdf[\"issue_d\"].notna().mean() >= 0.7:\n        temp = mdf[mdf[\"issue_d\"].notna()].copy()\n        cutoff = temp[\"issue_d\"].quantile(0.80)\n        train_idx = temp.index[temp[\"issue_d\"] <= cutoff]\n        test_idx  = temp.index[temp[\"issue_d\"] >  cutoff]\n        train_df = mdf.loc[train_idx].copy()\n        test_df  = mdf.loc[test_idx].copy()\n        # Attach NaT rows to train to avoid empties (ok for this task)\n        nat_rows = mdf.index[mdf[\"issue_d\"].isna()]\n        train_df = pd.concat([train_df, mdf.loc[nat_rows]], axis=0)\n        if len(train_df)>0 and len(test_df)>0 and train_df[\"default\"].nunique()==2:\n            return train_df, test_df\n    # Fallback\n    tr, te = train_test_split(mdf, test_size=0.2, random_state=42, stratify=mdf[\"default\"])\n    return tr.copy(), te.copy()\n\n# Robust trainer that works across d3rlpy versions\ndef robust_fit(algo, dataset, logdir=\"./logs_cql\", verbose=True):\n    attempts = []\n\n    # Most common modern signatures\n    attempts.append(lambda: algo.fit(dataset, n_epochs=20, logdir=logdir, verbose=verbose))\n    attempts.append(lambda: algo.fit(dataset, epochs=20, logdir=logdir, verbose=verbose))\n\n    # Positional-only epochs (older versions)\n    attempts.append(lambda: algo.fit(dataset, 20, logdir=logdir, verbose=verbose))\n    attempts.append(lambda: algo.fit(dataset, 20))\n\n    # Step-based training (some versions prefer total gradient steps)\n    attempts.append(lambda: algo.fit(dataset, n_steps=100_000, logdir=logdir, verbose=verbose))\n    attempts.append(lambda: algo.fit(dataset, total_steps=100_000, logdir=logdir, verbose=verbose))\n\n    # Last resort: minimal call\n    attempts.append(lambda: algo.fit(dataset))\n\n    last_err = None\n    for i, call in enumerate(attempts, 1):\n        try:\n            print(f\"[robust_fit] Trying signature #{i} ...\")\n            return call()\n        except TypeError as e:\n            print(f\"[robust_fit] Signature #{i} failed: {e}\")\n            last_err = e\n        except Exception as e:\n            print(f\"[robust_fit] Signature #{i} failed (other): {e}\")\n            last_err = e\n    raise last_err if last_err else RuntimeError(\"All fit() signatures failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:51:40.249302Z","iopub.execute_input":"2025-10-30T09:51:40.250326Z","iopub.status.idle":"2025-10-30T09:51:40.277183Z","shell.execute_reply.started":"2025-10-30T09:51:40.250285Z","shell.execute_reply":"2025-10-30T09:51:40.276269Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 4: Load and Split Data\ndf_clean = None\nif USE_CLEAN_FIRST and os.path.exists(CLEAN_PATH):\n    print(\"[Info] Using clean engineered file:\", CLEAN_PATH)\n    df_clean = pd.read_csv(CLEAN_PATH)\n    assert \"default\" in df_clean.columns, \"Clean file must contain 'default' column.\"\n\nif df_clean is None:\n    print(\"[Info] CLEAN not found → preprocessing raw:\", RAW_PATH)\n    hdr = pd.read_csv(RAW_PATH, compression=\"infer\", nrows=0, low_memory=True)\n    usecols = [c for c in FEATURES_RAW if c in hdr.columns]\n    df = pd.read_csv(RAW_PATH, compression=\"infer\", usecols=usecols, nrows=NROWS_FROM_RAW, low_memory=True)\n    print(\"Raw shape:\", df.shape)\n\n    if \"int_rate\" in df:   df[\"int_rate\"] = parse_pct(df[\"int_rate\"])\n    if \"revol_util\" in df: df[\"revol_util\"] = parse_pct(df[\"revol_util\"])\n    if \"emp_length\" in df: df[\"emp_length\"] = parse_emp_length(df[\"emp_length\"])\n    for dcol in (\"issue_d\",\"earliest_cr_line\"):\n        if dcol in df: df[dcol] = pd.to_datetime(df[dcol], format=\"%b-%Y\", errors=\"coerce\")\n\n    df[\"default\"] = df[\"loan_status\"].apply(map_target)\n    df = df[~df[\"default\"].isna()].copy()\n\n    # Build modeling frame\n    keep_cols = [c for c in df.columns if c != \"loan_status\"]\n    model_df = df[keep_cols].copy()\n    train_df, test_df = robust_time_split(model_df)\n\n    X_train_raw = train_df.drop(columns=[\"default\"])\n    X_test_raw  = test_df.drop(columns=[\"default\"])\n    y_train = train_df[\"default\"].astype(int).values\n    y_test  = test_df[\"default\"].astype(int).values\n\n    X_train = add_date_features(X_train_raw)\n    X_test  = add_date_features(X_test_raw)\n\nelse:\n    print(\"Clean file shape:\", df_clean.shape)\n    X = df_clean.drop(columns=[\"default\"])\n    y = df_clean[\"default\"].astype(int).values\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n    )\n\nprint(\"Train rows:\", len(y_train), \" Test rows:\", len(y_test))\ndisplay(X_train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T09:51:46.182238Z","iopub.execute_input":"2025-10-30T09:51:46.182552Z","iopub.status.idle":"2025-10-30T09:51:46.211501Z","shell.execute_reply.started":"2025-10-30T09:51:46.182518Z","shell.execute_reply":"2025-10-30T09:51:46.209888Z"}},"outputs":[{"name":"stdout","text":"[Info] CLEAN not found → preprocessing raw: accepted_2007_to_2018Q4.csv\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3474255000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf_clean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Info] CLEAN not found → preprocessing raw:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRAW_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFEATURES_RAW\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNROWS_FROM_RAW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'accepted_2007_to_2018Q4.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'accepted_2007_to_2018Q4.csv'","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# Cell 5: Define Preprocessing Pipeline\nnum_features = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\ncat_features = [c for c in X_train.columns if not pd.api.types.is_numeric_dtype(X_train[c])]\n\nnumeric_transformer = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\ncategorical_transformer = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n])\npreprocess = ColumnTransformer([\n    (\"num\", numeric_transformer, num_features),\n    (\"cat\", categorical_transformer, cat_features),\n])\n\nX_train_np = preprocess.fit_transform(X_train)\nX_test_np  = preprocess.transform(X_test)\n\nprint(\"Final feature dim:\", X_train_np.shape[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Build Offline RL Dataset\n# We create one-step transitions for BOTH actions per state:\n# - action=1 (Approve) → reward depends on outcome\n# - action=0 (Deny)    → reward = 0\n\ndef compute_reward(approve: int, default_flag: int, loan_amnt: float, int_rate: float) -> float:\n    if approve == 0:\n        return 0.0\n    # approve == 1\n    if default_flag == 0:  # fully paid\n        return float(loan_amnt) * float(int_rate) / 100.0  # int_rate is a percent value\n    else:  # defaulted\n        return -float(loan_amnt)\n\nassert \"loan_amnt\" in X_train.columns and \"int_rate\" in X_train.columns, \\\n    \"loan_amnt and int_rate must be present in features for the reward function.\"\n\n# Build transitions for TRAIN\nobs_train = []\nact_train = []\nrew_train = []\nter_train = []\n\nfor i in range(len(y_train)):\n    s = X_train_np[i]\n    la = X_train.iloc[i][\"loan_amnt\"]\n    ir = X_train.iloc[i][\"int_rate\"]\n    d  = int(y_train[i])\n\n    # action 0 (deny)\n    obs_train.append(s); act_train.append(0)\n    rew_train.append(compute_reward(0, d, la, ir)); ter_train.append(1.0)\n\n    # action 1 (approve)\n    obs_train.append(s); act_train.append(1)\n    rew_train.append(compute_reward(1, d, la, ir)); ter_train.append(1.0)\n\nobs_train = np.asarray(obs_train, dtype=np.float32)\nact_train = np.asarray(act_train, dtype=np.int64)\nrew_train = np.asarray(rew_train, dtype=np.float32)\nter_train = np.asarray(ter_train, dtype=np.float32)\n\ntrain_dataset = MDPDataset(\n    observations=obs_train,\n    actions=act_train,\n    rewards=rew_train,\n    terminals=ter_train,\n)\n\n# Build transitions for TEST (for evaluation simulation)\nobs_test = []\nact_test = []\nrew_test = []\nter_test = []\n\nfor i in range(len(y_test)):\n    s = X_test_np[i]\n    la = X_test.iloc[i][\"loan_amnt\"]\n    ir = X_test.iloc[i][\"int_rate\"]\n    d  = int(y_test[i])\n\n    # Same augmentation to allow greedy eval\n    obs_test.append(s); act_test.append(0)\n    rew_test.append(compute_reward(0, d, la, ir)); ter_test.append(1.0)\n\n    obs_test.append(s); act_test.append(1)\n    rew_test.append(compute_reward(1, d, la, ir)); ter_test.append(1.0)\n\nobs_test = np.asarray(obs_test, dtype=np.float32)\nact_test = np.asarray(act_test, dtype=np.int64)\nrew_test = np.asarray(rew_test, dtype=np.float32)\nter_test = np.asarray(ter_test, dtype=np.float32)\n\ntest_dataset = MDPDataset(\n    observations=obs_test,\n    actions=act_test,\n    rewards=rew_test,\n    terminals=ter_test,\n)\n\nn_train = obs_train.shape[0]\nn_test  = obs_test.shape[0]\n\nprint(f\"Train transitions: {n_train:,}\")\nprint(f\"Test transitions : {n_test:,}\")\nprint(f\"Unique train states (applications): {n_train // 2:,}\")\nprint(f\"Unique test  states (applications): {n_test  // 2:,}\")\nprint(\"obs_train shape:\", obs_train.shape, \" actions:\", act_train.shape,\n      \" rewards:\", rew_train.shape, \" terminals:\", ter_train.shape)\nprint(\"obs_test  shape:\", obs_test.shape,  \" actions:\", act_test.shape,\n      \" rewards:\", rew_test.shape,  \" terminals:\", ter_test.shape)\n\ntry:\n    print(\"train_dataset.size():\", train_dataset.size())\n    print(\"test_dataset.size():\",  test_dataset.size())\nexcept Exception:\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Define Algorithm\n# This cell was added to define the CQL algorithm.\nalgo = d3rlpy.algos.DiscreteCQL(\n    batch_size=32,\n    learning_rate=6.25e-05,\n    use_gpu=False # Set to True if a GPU is available\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Train Algorithm\n# This calls the robust_fit function defined in Cell 3\nrobust_fit(algo, train_dataset, logdir=\"./logs_cql\", verbose=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Evaluate Policy Offline on Test Set\n# Greedy action per state: a_hat = argmax_a Q(s,a)\n# Then compute the realized reward using the ground-truth label.\n\ndef greedy_action(algo: CQL, states: np.ndarray) -> np.ndarray:\n    # d3rlpy's predict expects batch of observations\n    return algo.predict(states)\n\n# Deduplicate the paired (s, a=0) and (s, a=1) in test_dataset\nunique_states = obs_test[::2]   # every two entries share the same state\nassert unique_states.shape[0] == len(y_test)\n\na_hat = greedy_action(algo, unique_states)  # 0 or 1 per state\n\n# Compute rewards under your rule\nrewards = []\napproved_defaults = 0\napproved_paid = 0\n\nfor i in range(len(y_test)):\n    la = X_test.iloc[i][\"loan_amnt\"]\n    ir = X_test.iloc[i][\"int_rate\"]\n    d  = int(y_test[i])\n    ah = int(a_hat[i])\n\n    r = compute_reward(ah, d, la, ir)\n    rewards.append(r)\n\n    if ah == 1:\n        if d == 1: approved_defaults += 1\n        else:      approved_paid += 1\n\nrewards = np.array(rewards, dtype=float)\n\navg_reward = rewards.mean()\napproval_rate = float((a_hat == 1).mean())\n\nprint(f\"Avg reward per application: {avg_reward:,.2f}\")\nprint(f\"Approval rate: {approval_rate*100:.2f}%\")\nprint(f\"Approved & Fully Paid count: {approved_paid}\")\nprint(f\"Approved & Defaulted count : {approved_defaults}\")\n\n# Confusion-style table for approvals on test\ndeny_paid = ((a_hat == 0) & (y_test == 0)).sum()\ndeny_def  = ((a_hat == 0) & (y_test == 1)).sum()\napp_paid  = ((a_hat == 1) & (y_test == 0)).sum()\napp_def   = ((a_hat == 1) & (y_test == 1)).sum()\n\nprint(\"\\nDecision vs Outcome (test):\")\nprint(pd.DataFrame(\n    [[deny_paid, deny_def],\n     [app_paid,  app_def]],\n    index=[\"Deny\",\"Approve\"],\n    columns=[\"Paid\",\"Default\"]\n))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Save Artifacts\nART_DIR = \"/content/offline_rl_cql\"\nos.makedirs(ART_DIR, exist_ok=True)\n\n# Save d3rlpy model\nalgo.save(os.path.join(ART_DIR, \"cql_discrete_model.d3\"))\n\n# Save the sklearn preprocess pipeline\nimport joblib\njoblib.dump(preprocess, os.path.join(ART_DIR, \"preprocess.joblib\"))\n\n# Save a quick report\nwith open(os.path.join(ART_DIR, \"report.txt\"), \"w\") as f:\n    f.write(textwrap.dedent(f\"\"\"\n    Offline RL — CQL (Discrete) — One-step Loan Approval\n    Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n    Avg reward per application (test): {avg_reward:,.4f}\n    Approval rate (test)              : {approval_rate*100:.2f}%\n    Approved & Fully Paid (count)     : {approved_paid}\n    Approved & Defaulted (count)      : {approved_defaults}\n    \"\"\").strip())\n\nprint(\"Saved:\")\nprint(\" -\", os.path.join(ART_DIR, \"cql_discrete_model.d3\"))\nprint(\" -\", os.path.join(ART_DIR, \"preprocess.joblib\"))\nprint(\" -\", os.path.join(ART_DIR, \"report.txt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}